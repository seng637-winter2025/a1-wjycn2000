**SENG 637 \- Software Testing, Reliability, and Quality**

**Lab. Report \#1 – Introduction to Testing and Defect Tracking**

| Group: 18 |
| :---- |
| Abubakar Khalid |
| Mo Salah |
| Ahmed |
| Jinyu |

**Table of Contents**

[Introduction](#introduction)

[High-level description of the exploratory testing plan](#high-level-description-of-the-exploratory-testing-plan)

[Test Plan](#test-plan)

[Test types](#test-types)

[Scope of testing](#scope-of-testing)

[Features to be tested](#features-to-be-tested)

[Features not to be tested](#features-not-to-be-tested)

[Test Logistics](#test-logistics)

[Comparison of exploratory and manual functional testing](#comparison-of-exploratory-and-manual-functional-testing)

[Notes and discussion of the peer reviews of defect reports](#notes-and-discussion-of-the-peer-reviews-of-defect-reports)

[How the pair testing was managed and team work/effort was divided](#how-the-pair-testing-was-managed-and-team-work/effort-was-divided)

[Difficulties encountered, challenges overcome, and lessons learned](#difficulties-encountered,-challenges-overcome,-and-lessons-learned)

[Comments/feedback on the lab and lab document itself](#comments/feedback-on-the-lab-and-lab-document-itself)

# Introduction {#introduction}

Before starting this lab, we had a very minimal or you can say basic understanding of software testing, but none of us had worked with exploratory or manual functional testing in a structured way. The name sort of gave it away but we knew that exploratory testing involves freely interacting with a system to find unexpected issues, while manual functional testing (MFT) follows a set of predefined test cases to verify expected behavior. However, we had never applied these concepts in a real testing environment, nor did we have any prior experience using JIRA for defect tracking

This lab gave us hands-on experience with both approaches. We started with exploratory testing, where we tested the ATM system without a script, trying different inputs and scenarios to uncover potential bugs. After that, we moved on to manual functional testing, where we followed a structured test plan and logged any issues in JIRA. Finally, we conducted regression testing on the updated version of the ATM system to verify which bugs were fixed and which still existed.

Through this process, we gained a much better understanding of how real-world testing works, how to properly document and track defects, and how different testing methods contribute to improving software quality.

# High-level description of the exploratory testing plan {#high-level-description-of-the-exploratory-testing-plan}

For our exploratory testing, we approached the ATM simulation system with the goal of identifying unexpected behaviors and system weaknesses. Since this type of testing does not follow a strict script, we focused on realistic user interactions and edge cases that could potentially reveal defects. We began by navigating through the system to understand its basic functionality and overall flow. This included powering on the ATM, inserting a card, and attempting standard transactions. Once we had a general understanding, we tested different authentication scenarios, such as entering valid and invalid card numbers and PINs, including exceeding the maximum number of incorrect attempts to see how the system handled card retention.

Next, we explored various transaction processes, including withdrawals, deposits, transfers, and balance inquiries, ensuring they functioned as expected. We also tested edge cases by deliberately entering invalid values, exceeding system limits, and performing transactions in an unusual sequence to observe how the system responded. A key part of our testing involved assessing the system's error handling and stability by triggering conditions such as insufficient funds, invalid account types, and interruptions during transactions.

Each group member was assigned specific areas to explore to ensure thorough coverage of the system. Any unusual or incorrect behavior was immediately documented and later reported in JIRA with detailed defect logs. Bugs found during exploratory testing were later validated through manual scripted testing (MFT) to confirm whether they were repeatable issues. By the end of this process, we had identified several critical and unexpected defects that might not have been immediately noticeable through scripted testing alone. This step reinforced the importance of unscripted testing in real-world software validation and provided valuable insights into the robustness of the system.

## Test Plan {#test-plan}

## Test types {#test-types}

For this lab, we performed three types of testing to evaluate the ATM simulation system: exploratory testing, manual functional testing (MFT), and regression testing. Exploratory testing was conducted without predefined test cases, allowing testers to freely interact with the system to uncover unexpected issues. The primary goal was to identify edge cases, usability concerns, and unanticipated behaviors. Manual functional testing (MFT), on the other hand, involved executing structured test cases based on a predefined test suite to verify that the system behaved as expected. Each test case followed a step-by-step process with specific expected outcomes and pass/fail criteria. Finally, regression testing was conducted after receiving the updated ATM System v1.1. We retested previously identified defects to determine whether they had been fixed. If a defect was resolved, we updated its status in JIRA as "Resolved." If the issue persisted, we marked it "In Progress" and added comments explaining the continued failure. Additionally, we re-executed the MFT test cases to check for any new defects introduced in version 1.1.

## Scope of testing {#scope-of-testing}

The scope of this test plan was to evaluate the core functionalities of the ATM system, focusing on key areas of user interaction and transaction processing. The first area of testing involved authentication and card handling, where we tested valid and invalid card entries, correct and incorrect PIN inputs, incorrect PIN retries, and card retention behavior when a user failed authentication multiple times. We also tested cash withdrawal functionalities, ensuring that the ATM correctly processed withdrawals for both sufficient and insufficient account balances while verifying proper cash dispensing behavior. Another critical area was deposit transactions, where we tested both valid and invalid deposit attempts, handling of deposit envelopes, and cases where deposits were canceled or failed due to system constraints.

Additionally, we tested fund transfers to validate transactions between different account types and checked how the system handled invalid account selections. Balance inquiry tests ensured that users could retrieve accurate balance information from their accounts, while also confirming that account balances updated correctly after transactions. Finally, we conducted transaction cancellation tests to verify that users could cancel transactions at different stages without causing system errors or unexpected behavior. By covering these functionalities, we ensured that our testing approach addressed both critical system operations and edge cases that could impact real-world usage.

### Features to be tested {#features-to-be-tested}

| Function | Description |
| :---- | :---- |
| Authentication | Customer insert a card and enter a correct PIN to successfully log in |
| Wrong PIN is not accepted | Customer insert a card and enter an incorrect PIN and is rejected and asked to enter PIN again |
| Enter wrong PIN 3 times | Customer insert a card and enter wrong PIN 3 times and system will retain the card |
| Withdraw money under current balance | After successful logging in, customer withdraw $50 from checking account and the system should print receipt showing correct balance |
| Withdraw money over current balance | After successful logging in, customer withdraw $200 from checking account and the system should prompt not enough balance |
| Withdraw money over cash in ATM | The ATM should be started with 5 $20 bills. After successful logging in, customer withdraw $200 from checking account and the system should fail to proceed |
| Deposit money to checking account | After successful logging in, customer deposit $200 to checking account |
| Deposit money to savings account | After successful logging in, customer deposit $200 to savings account |
| Transfer money from checking to savings | After successful logging in, customer transfer $20 from checking to saving, and system should print receipt showing successful transaction and correct details |
| Transfer money from savings to checking | After successful logging in, customer transfer $20 from checking to saving, and system should print receipt showing successful transaction and correct details |
| Transfer money from checking to savings but money is more than current balance in checking | After successful logging in, customer transfer $200 from checking to saving, and system should fail to proceed |
| Inquire balance of checking | After successful logging in, customer inquire balance of checking account and system should print current balance |

### Features not to be tested {#features-not-to-be-tested}

Invalid input for the number of $20 bills when ATM is switched on  
Pressing buttons that do not correspond to any menu item to see if there is an effect

## Test Logistics {#test-logistics}

To efficiently execute our test plan, we divided our team of four into two pairs. Each pair conducted exploratory testing and manual functional testing (MFT) independently before coming together to compare findings and log defects in JIRA. During exploratory testing, one member in each pair acted as the tester, executing tests and sharing their screen, while the other member observed and recorded defects in real-time. One pair primarily focused on authentication and card handling, while the other pair concentrated on transaction-related functions, such as withdrawals, deposits, transfers, and balance inquiries. Once exploratory testing was complete, we reviewed our results as a group to validate defects before officially logging them.

For manual functional testing (MFT), each pair executed all 40 test cases, but within the pair, the workload was evenly split between the two members to improve efficiency. This ensured that each test case was executed twice independently, once by each pair, allowing for cross-verification of results. Each pair documented test results, reported defects in JIRA, and tagged them under "MFT." After both pairs completed testing, we compared our findings to confirm consistency across test results and ensure that all significant issues were captured.

For regression testing, each team member retested the defects they originally reported in version 1.0 to determine whether they were resolved in ATM System v1.1. If a defect was fixed, we updated its status in JIRA as "Resolved." If it still persisted, we marked it as "In Progress" and added comments explaining the issue. After verifying previously reported defects, both pairs re-executed their MFT test cases to check for any newly introduced defects. By structuring our testing approach this way, we ensured comprehensive test coverage, multiple verification layers, and an efficient defect tracking process using JIRA.

# Comparison of exploratory and manual functional testing {#comparison-of-exploratory-and-manual-functional-testing}

Exploratory testing and manual functional testing (MFT) both play crucial roles in software quality assurance, but they differ significantly in their approach, structure, and effectiveness in identifying defects. Through this lab, we were able to directly compare these two testing methodologies and understand their strengths and trade-offs.

Exploratory testing was unscripted and flexible, allowing us to interact with the system freely and test various edge cases without predefined test cases. This approach helped us uncover unexpected bugs that may not have been considered in a structured test plan. For example, we discovered issues related to incorrect balance updates after deposits and transfers, which were not explicitly listed as test cases but were found through exploratory interactions. However, one of the main drawbacks of exploratory testing was lack of consistency and repeatability—since different testers explored different aspects of the system, not every issue was guaranteed to be discovered, and some defects might have been overlooked due to the lack of structure. Additionally, exploratory testing requires domain knowledge of the system being tested. For example, knowing the functions of an ATM and its typical weaknesses helped us identify potential problem areas faster.

In contrast, manual functional testing (MFT) was highly structured and repeatable, following a predefined set of test cases to verify specific functionalities of the ATM system. This method ensured that all core features were tested systematically, and every function was evaluated against its expected behavior. One major advantage of MFT is that steps are well-documented, making it easier to reproduce issues and track them effectively. Additionally, since MFT follows a structured plan, it does not require deep domain knowledge—testers simply need to know how to operate the ATM and execute the given test cases. However, one drawback of MFT is that it may miss unexpected defects, as it only tests predefined scenarios rather than allowing for spontaneous issue discovery like exploratory testing.

Another key difference is efficiency. Exploratory testing can quickly find critical issues since testers are free to interact with the system in a way that mimics real-world usage. However, manual functional testing is slower since it must follow a predefined sequence of test cases. Despite being more time-consuming, MFT is better suited for regression testing, as it allows testers to systematically retest known issues and verify fixes in an updated version of the software.

A key takeaway from this lab was that both testing approaches are necessary for thorough software testing. Exploratory testing is great for discovering unexpected issues and testing the system from a real user’s perspective, while MFT ensures that the core functionality works as intended and provides structured, repeatable results. By combining both methods, we were able to maximize defect detection and ensure comprehensive test coverage for the ATM system.

# Notes and discussion of the peer reviews of defect reports {#notes-and-discussion-of-the-peer-reviews-of-defect-reports}

As part of the lab, we conducted peer reviews of the defect reports submitted by both pairs in our group. The goal of the peer review process was to ensure accuracy, consistency, and completeness in how defects were documented in JIRA. This step helped us refine our reports, correct any missing details, and gain a better understanding of how to improve our defect descriptions.

During the review, we examined whether the summary and description of each defect were clear and easy to understand. Some initial reports lacked specificity in describing the issue, so we revised them to include detailed steps to reproduce the bug along with clear expected and actual results. A defect is only useful if it can be reproduced by others, so we focused on ensuring that all reports contained explicit, step-by-step instructions. Some reports were missing key steps required to trigger the bug, which led to confusion when retesting them during regression testing. Addressing these gaps ensured that anyone reviewing the defects could replicate them without ambiguity.

Another key aspect of the review was maintaining consistency in severity and priority classification. Some defects were initially assigned different severity ratings by different pairs. For example, a bug causing a miscalculated balance after a deposit was originally labeled as "Medium" severity by one pair, while the other pair marked a similar issue as "High." To ensure uniformity, we discussed these discrepancies and aligned our severity ratings based on the actual impact of each bug on system functionality.

In addition to severity classification, we reviewed the labeling and categorization of defects. Some reports were missing appropriate labels indicating the affected functionality, such as transactions, balance inquiry, or authentication. By adding these labels in JIRA, we made it easier to track defects by category, helping ensure that related issues were grouped together for efficient resolution.

The review process also emphasized the importance of correctly updating defect statuses during regression testing. When we tested ATM System v1.1, we updated the reports to indicate which bugs had been resolved and which remained unresolved. Some reports lacked proper status updates, leading to confusion about whether an issue was still present. We ensured that defects marked as fixed were updated to "Resolved," while those still present were labeled "In Progress," with comments explaining their status.

Overall, the peer review process significantly improved the quality of our defect reports, making them more structured, reproducible, and aligned with industry standards. It reinforced the importance of clear documentation in software testing, as missing or vague details could lead to confusion in later testing stages. This process provided valuable insight into how professional QA teams collaborate to refine defect reports, ensuring clarity and accuracy for efficient debugging and resolution.

# How the pair testing was managed and team work/effort was divided {#how-the-pair-testing-was-managed-and-team-work/effort-was-divided}

To ensure a structured and efficient approach to testing, we followed the professor’s requirement of working in pairs within our group of four. Each pair conducted both exploratory testing and manual functional testing (MFT) independently before coming together to compare findings and log defects in JIRA. This ensured that all identified issues were validated and properly documented.

For exploratory testing, each pair tested the ATM simulation system separately, with one member acting as the tester, executing different test cases and sharing their screen, while the other member observed and recorded defects. The recorder was also responsible for logging the identified bugs into JIRA in real-time to ensure accuracy. One pair focused on authentication and card handling, testing various scenarios such as valid and invalid card entries, incorrect PIN attempts, and card retention behavior. The second pair concentrated on transaction-related functions, including withdrawals, deposits, transfers, and balance inquiries. Once both pairs completed their exploratory testing, we compared our results, reviewed inconsistencies, and ensured that all defects were thoroughly discussed before finalizing the logs in JIRA.

For manual scripted testing (MFT), each pair was responsible for executing all 40 test cases. However, to distribute the workload efficiently, the test cases were evenly split within each pair, meaning that while each pair covered all 40 cases, individual members focused on half of them. This setup ensured that every test case was executed twice independently—once by each pair—allowing for cross-verification of defects. Each pair followed the structured test suite, recorded results, and logged any discovered bugs in JIRA under the "MFT" tag. After completing the MFT phase, we compared our findings, confirming which defects were consistently reproducible across both pairs before finalizing the defect reports.

For regression testing, each team member was responsible for retesting the defects they originally reported in version 1.0 to verify whether they had been fixed in ATM System v1.1. If a defect was resolved, its status in JIRA was updated to "Resolved," whereas defects that still persisted were marked as "In Progress" with additional comments detailing the issue. After verifying previous defects, both pairs re-executed their assigned MFT test cases from version 1.0 to check for any newly introduced defects in version 1.1.

By structuring our testing approach in this way, we ensured a thorough evaluation of the ATM system while adhering to the required pair-testing methodology. With multiple testers verifying each function independently, we maximized test coverage, improved the reliability of our findings, and effectively tracked all defects using

# Difficulties encountered, challenges overcome, and lessons learned {#difficulties-encountered,-challenges-overcome,-and-lessons-learned}

Throughout this lab, we encountered several challenges that tested our problem-solving abilities and teamwork. One of the biggest difficulties was understanding the structure of the testing process itself. Initially, we were confused about the differences between exploratory testing, manual functional testing (MFT), and regression testing, as well as how to properly document and track defects in JIRA. However, through trial and error, discussions within our group, and cross-referencing the assignment instructions, we were able to gain clarity on each phase and streamline our workflow.

Another challenge we faced was ensuring consistency in defect reporting. Since each pair conducted their own testing, it was crucial that we logged defects clearly and systematically so that they could be properly tracked and retested during regression testing. At first, we noticed inconsistencies in how defects were described, with some reports missing key details. To overcome this, we standardized our bug reporting format in JIRA, ensuring that every defect included steps to reproduce, expected vs. actual results, severity, and affected version. This not only made our reports clearer but also prepared us for real-world defect tracking in professional environments.

A technical challenge we encountered was identifying whether certain issues were actual bugs or expected system behavior. For example, some transactions failed without an error message, while others deducted incorrect amounts from the balance. We had to refer back to the system requirements to confirm whether the behavior was a defect or intended functionality. This reinforced the importance of understanding system specifications before testing and asking the right questions when identifying issues.

Working with JIRA was another learning curve. Many of us had never used a bug tracking system before, so there was an adjustment period where we had to figure out how to properly create issues, assign statuses, and update defect logs. Over time, we became comfortable with JIRA’s workflow, realizing how powerful it is for tracking software defects in an organized and collaborative manner.

One of the most valuable lessons we learned was the importance of pair testing and teamwork. By dividing the workload and verifying each other’s findings, we were able to catch more defects, prevent oversight, and ensure a higher level of accuracy in our testing. We also gained real-world experience in structured testing methodologies, reinforcing the importance of both scripted and unscripted testing approaches in software quality assurance.

Ultimately, this lab helped us develop a deeper understanding of software testing, bug tracking, and collaboration. The challenges we faced were valuable learning experiences that improved our ability to analyze, document, and validate software defects effectively.

# Comments/feedback on the lab and lab document itself {#comments/feedback-on-the-lab-and-lab-document-itself}

Overall, this lab provided a great hands-on introduction to software testing and defect tracking. The structured process of exploratory testing, manual functional testing (MFT), and regression testing helped us understand different testing approaches and how to log defects systematically in JIRA. Working with JIRA was especially valuable since it's a widely used industry tool, and this lab gave us a chance to learn its workflow in a real testing scenario. However, despite the learning experience, there were several areas where the lab could have been more structured and clearer.

One of the biggest frustrations we faced was the lack of clarity on what exactly needed to be submitted. At multiple points, we were unsure whether we had to submit just the JIRA bug reports, a Markdown file, a PDF, or all of them together. The instructions were not always structured well, and we found ourselves second-guessing what was required and in what format. This caused unnecessary confusion and made it difficult to focus on the actual testing. A clearer breakdown of submission requirements with specific instructions on what files to include would have saved us a lot of time and effort.

Another issue was the uncertainty around how to divide the work within the group. The lab mentioned pair testing, but it wasn’t entirely clear whether each pair was supposed to complete the full set of test cases independently or if we were meant to split the workload differently. We ended up making a decision based on what seemed most logical, but some clearer guidance on how to coordinate testing within a team would have been helpful. Additionally, some expected behaviors of the ATM system were unclear, making it difficult to determine whether certain issues were actual bugs or just system limitations. Since there wasn’t much documentation explaining the expected outcomes for certain edge cases, we had to make assumptions when logging issues in JIRA. A more detailed system specification would have helped us confidently differentiate between intended functionality and actual defects.

Despite these frustrations, the lab was a valuable learning experience. It reinforced the importance of structured testing, defect tracking, and how to work collaboratively in a testing environment. However, improving the clarity of submission requirements, refining the instructions on group work distribution, and providing more details on the ATM system’s expected behavior would make the lab much smoother and more effective for future students.